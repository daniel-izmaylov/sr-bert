[
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "features",
        "importPath": "pyexpat",
        "description": "pyexpat",
        "isExtraImport": true,
        "detail": "pyexpat",
        "documentation": {}
    },
    {
        "label": "features",
        "importPath": "pyexpat",
        "description": "pyexpat",
        "isExtraImport": true,
        "detail": "pyexpat",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "MSELoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "NLLLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BCELoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "WEIGHTS_NAME",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertForSequenceClassification",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "top_k_top_p_filtering",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertForPreTraining",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertPreTrainedModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertLMHeadModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "WEIGHTS_NAME",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertForSequenceClassification",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertForSequenceClassification",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "WEIGHTS_NAME",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "path",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "BertPredictionHeadTransform",
        "importPath": "transformers.models.bert.modeling_bert",
        "description": "transformers.models.bert.modeling_bert",
        "isExtraImport": true,
        "detail": "transformers.models.bert.modeling_bert",
        "documentation": {}
    },
    {
        "label": "BertForSequenceClassification_v2",
        "importPath": "modules",
        "description": "modules",
        "isExtraImport": true,
        "detail": "modules",
        "documentation": {}
    },
    {
        "label": "BertForSequenceClassification_lex_features",
        "importPath": "modules",
        "description": "modules",
        "isExtraImport": true,
        "detail": "modules",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_auc_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "fbeta_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "ConfusionMatrixDisplay",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "DialogBert_Infernce",
        "importPath": "inference",
        "description": "inference",
        "isExtraImport": true,
        "detail": "inference",
        "documentation": {}
    },
    {
        "label": "DialogTransformerDataset",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "DialogTransformerDataset",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "HBertMseEuopDataset",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "DialogTransformerDatasetClassification",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "load_dict",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "save_vecs",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "load_dict",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "save_vecs",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "load_dict",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "save_vecs",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "DialogTransformerDataset",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "load_vecs",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "load_dict",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "save_vecs",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "DialogTransformerDataset",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "HBertMseEuopDataset",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "DialogTransformerDatasetClassification",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "tables",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tables",
        "description": "tables",
        "detail": "tables",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "shuffle",
        "importPath": "sklearn.utils",
        "description": "sklearn.utils",
        "isExtraImport": true,
        "detail": "sklearn.utils",
        "documentation": {}
    },
    {
        "label": "resample",
        "importPath": "sklearn.utils",
        "description": "sklearn.utils",
        "isExtraImport": true,
        "detail": "sklearn.utils",
        "documentation": {}
    },
    {
        "label": "shuffle",
        "importPath": "sklearn.utils",
        "description": "sklearn.utils",
        "isExtraImport": true,
        "detail": "sklearn.utils",
        "documentation": {}
    },
    {
        "label": "applyparallel",
        "importPath": "multiprocesspandas",
        "description": "multiprocesspandas",
        "isExtraImport": true,
        "detail": "multiprocesspandas",
        "documentation": {}
    },
    {
        "label": "applyparallel",
        "importPath": "multiprocesspandas",
        "description": "multiprocesspandas",
        "isExtraImport": true,
        "detail": "multiprocesspandas",
        "documentation": {}
    },
    {
        "label": "applyparallel",
        "importPath": "multiprocesspandas",
        "description": "multiprocesspandas",
        "isExtraImport": true,
        "detail": "multiprocesspandas",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "DialogBERTSolver",
        "importPath": "solvers",
        "description": "solvers",
        "isExtraImport": true,
        "detail": "solvers",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "gc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gc",
        "description": "gc",
        "detail": "gc",
        "documentation": {}
    },
    {
        "label": "models,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "models.",
        "description": "models.",
        "detail": "models.",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data.distributed",
        "description": "torch.utils.data.distributed",
        "isExtraImport": true,
        "detail": "torch.utils.data.distributed",
        "documentation": {}
    },
    {
        "label": "sentence_bleu",
        "importPath": "nltk.translate.bleu_score",
        "description": "nltk.translate.bleu_score",
        "isExtraImport": true,
        "detail": "nltk.translate.bleu_score",
        "documentation": {}
    },
    {
        "label": "SmoothingFunction",
        "importPath": "nltk.translate.bleu_score",
        "description": "nltk.translate.bleu_score",
        "isExtraImport": true,
        "detail": "nltk.translate.bleu_score",
        "documentation": {}
    },
    {
        "label": "meteor_score",
        "importPath": "nltk.translate.meteor_score",
        "description": "nltk.translate.meteor_score",
        "isExtraImport": true,
        "detail": "nltk.translate.meteor_score",
        "documentation": {}
    },
    {
        "label": "sentence_nist",
        "importPath": "nltk.translate.nist_score",
        "description": "nltk.translate.nist_score",
        "isExtraImport": true,
        "detail": "nltk.translate.nist_score",
        "documentation": {}
    },
    {
        "label": "log_model",
        "importPath": "comet_ml.integration.pytorch",
        "description": "comet_ml.integration.pytorch",
        "isExtraImport": true,
        "detail": "comet_ml.integration.pytorch",
        "documentation": {}
    },
    {
        "label": "forward",
        "importPath": "turtle",
        "description": "turtle",
        "isExtraImport": true,
        "detail": "turtle",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "torch.nn.init",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "pack_padded_sequence",
        "importPath": "torch.nn.utils.rnn",
        "description": "torch.nn.utils.rnn",
        "isExtraImport": true,
        "detail": "torch.nn.utils.rnn",
        "documentation": {}
    },
    {
        "label": "pad_packed_sequence",
        "importPath": "torch.nn.utils.rnn",
        "description": "torch.nn.utils.rnn",
        "isExtraImport": true,
        "detail": "torch.nn.utils.rnn",
        "documentation": {}
    },
    {
        "label": "PriorityQueue",
        "importPath": "queue",
        "description": "queue",
        "isExtraImport": true,
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "operator",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "operator",
        "description": "operator",
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "SequenceClassifierOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "importPath": "sklearn.metrics.pairwise",
        "description": "sklearn.metrics.pairwise",
        "isExtraImport": true,
        "detail": "sklearn.metrics.pairwise",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "tensorboardX",
        "description": "tensorboardX",
        "isExtraImport": true,
        "detail": "tensorboardX",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "models",
        "description": "models",
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "SR_BERT",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "Learner",
        "importPath": "learner",
        "description": "learner",
        "isExtraImport": true,
        "detail": "learner",
        "documentation": {}
    },
    {
        "label": "SSK_Task",
        "kind": 6,
        "importPath": "models.SR_BERT",
        "description": "models.SR_BERT",
        "peekOfCode": "class SSK_Task(nn.Module):\n    \"\"\" A Self Sorting Network which evaluates the sorting energy of each element in a sequence, adopted from the self-attention.\n    Args:\n        dimensions (int): Dimensionality of the sequence\n    Example:\n         >>> sortnet = SelfSorting(256)\n         >>> seq = torch.randn(6, 5, 256)\n         >>> sorting_scores = sortnet(seq)\n         >>> sorting_scores.size()\n         torch.Size([6, 5])",
        "detail": "models.SR_BERT",
        "documentation": {}
    },
    {
        "label": "SelfSorting",
        "kind": 6,
        "importPath": "models.SR_BERT",
        "description": "models.SR_BERT",
        "peekOfCode": "class SelfSorting(nn.Module):\n    \"\"\" A Self Sorting Network which evaluates the sorting energy of each element in a sequence, adopted from the self-attention.\n    Args:\n        dimensions (int): Dimensionality of the sequence\n    Example:\n         >>> sortnet = SelfSorting(256)\n         >>> seq = torch.randn(6, 5, 256)\n         >>> sorting_scores = sortnet(seq)\n         >>> sorting_scores.size()\n         torch.Size([6, 5])",
        "detail": "models.SR_BERT",
        "documentation": {}
    },
    {
        "label": "SR_BERT",
        "kind": 6,
        "importPath": "models.SR_BERT",
        "description": "models.SR_BERT",
        "peekOfCode": "class SR_BERT(nn.Module):\n    '''Hierarchical BERT for dialog v5 with two features:\n    - Masked context utterances prediction with direct MSE matching of their vectors\n    - Energy-based Utterance order prediction: A novel approach to shuffle the context and predict the original order with distributed order prediction'''\n    def __init__(self):\n        super(SR_BERT, self).__init__()  \n    def __init__(self, args, base_model_name='bert-base-uncased'):\n        super(SR_BERT, self).__init__()  \n        if args.language == 'chinese': base_model_name = 'bert-base-chinese'\n        elif args.language == 'hebrew': base_model_name = 'onlplab/alephbert-base'",
        "detail": "models.SR_BERT",
        "documentation": {}
    },
    {
        "label": "listNet",
        "kind": 2,
        "importPath": "models.SR_BERT",
        "description": "models.SR_BERT",
        "peekOfCode": "def listNet(scores_pred, perm_true, pad_mask, eps=1e-3):\n    \"\"\"\n    ListMLE loss introduced in \"Listwise Approach to Learning to Rank - Theory and Algorithm\".\n    Adopted from https://github.com/allegro/allRank/blob/master/allrank/models/losses/listNet.py\n    :param scores_pred: predicted sorting scores from the model, shape [batch_size, seq_length]\n    :param perm_true: ground truth order, shape [batch_size, seq_length]\n    :param eps: epsilon value, used for numerical stability\n    :param pad_mask: an indicator of the perm_true containing a padded item\n    :return: loss value, a torch.Tensor\n    \"\"\"",
        "detail": "models.SR_BERT",
        "documentation": {}
    },
    {
        "label": "listMLE",
        "kind": 2,
        "importPath": "models.SR_BERT",
        "description": "models.SR_BERT",
        "peekOfCode": "def listMLE(scores_pred, perm_true, pad_mask, eps=1e-10):\n    \"\"\"\n    ListMLE loss introduced in \"Listwise Approach to Learning to Rank - Theory and Algorithm\".\n    Adopted from https://github.com/allegro/allRank/blob/master/allrank/models/losses/listMLE.py\n    :param scores_pred: predicted sorting scores from the model, shape [batch_size, seq_length]\n    :param perm_true: ground truth order, shape [batch_size, seq_length]\n    :param eps: epsilon value, used for numerical stability\n    :param pad_mask: an indicator of the perm_true containing a padded item\n    :return: loss value, a torch.Tensor\n    \"\"\"",
        "detail": "models.SR_BERT",
        "documentation": {}
    },
    {
        "label": "parentPath",
        "kind": 5,
        "importPath": "models.SR_BERT",
        "description": "models.SR_BERT",
        "peekOfCode": "parentPath = os.path.abspath(\"..\")\nsys.path.insert(0, parentPath)# add parent folder to path so as to import common modules\nfrom transformers import (top_k_top_p_filtering,BertConfig, BertForPreTraining, BertPreTrainedModel, BertTokenizer, BertModel, BertLMHeadModel)\nfrom transformers.models.bert.modeling_bert import BertPredictionHeadTransform\n# from modules import Classification_head\nfrom modules import BertForSequenceClassification_v2, BertForSequenceClassification_lex_features\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\n# from modules import MLP, MixtureDensityNetwork\nclass SSK_Task(nn.Module):",
        "detail": "models.SR_BERT",
        "documentation": {}
    },
    {
        "label": "DialogTransformerDatasetClassification_parts_withKnowalge",
        "kind": 6,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "class DialogTransformerDatasetClassification_parts_withKnowalge(DialogTransformerDataset):\n    \"\"\"\n    A base class for Transformer dataset\n    \"\"\"\n    def __init__(self, file_path, tokenizer, \n                 min_num_utts=1, max_num_utts=7,percentage_utts=1.0, max_utt_len=30, \n                 block_size=256, utt_masklm=False, utt_sop=False, \n                 context_shuf=False, context_masklm=False,turns=False):\n        # 1. Initialize file path or list of file names.\n        \"\"\"read training sentences(list of int array) from a hdf5 file\"\"\"",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "perf_measure",
        "kind": 2,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "def perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "src_file_path",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "src_file_path = inspect.getfile(lambda: None)\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(src_file_path))))\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "alephbert_tokenizer",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "alephbert_tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\ntemp=[]\nfor convs in text:\n    for conv in convs:\n        temp.append(list(map(lambda p: alephbert_tokenizer.decode(p, skip_special_tokens=True), conv)))\nconvs=temp \ndel temp\n# list(map(lambda p: alephbert_tokenizer.decode(p, skip_special_tokens=True), convs[0]))\nimport pandas as pd\nconv_df=pd.DataFrame()",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "cf_matrix",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "cf_matrix = confusion_matrix(conv_df[\"labels\"], conv_df[\"preds\"],normalize='true')\nax = sns.heatmap(cf_matrix, annot=True, \n            fmt='.2%', cmap='Blues')\nax.set_title('SRF-Lexicon\\n\\n');\nax.set_xlabel('\\nPredicted Values')\nax.set_ylabel('Actual Values ');\n## Ticket labels - List must be in alphabetical order\nax.xaxis.set_ticklabels(['False','True'])\nax.yaxis.set_ticklabels(['False','True'])\nplt.show()",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "ax = sns.heatmap(cf_matrix, annot=True, \n            fmt='.2%', cmap='Blues')\nax.set_title('SRF-Lexicon\\n\\n');\nax.set_xlabel('\\nPredicted Values')\nax.set_ylabel('Actual Values ');\n## Ticket labels - List must be in alphabetical order\nax.xaxis.set_ticklabels(['False','True'])\nax.yaxis.set_ticklabels(['False','True'])\nplt.show()\ndef perf_measure(y_actual, y_hat):",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "TPR",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "TPR = TP/(TP+FN)\n# Specificity or true negative rate\nTNR = TN/(TN+FP) \n# Precision or positive predictive value\nPPV = TP/(TP+FP)\n# Negative predictive value\nNPV = TN/(TN+FN)\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\n# False negative rate",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "TNR",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "TNR = TN/(TN+FP) \n# Precision or positive predictive value\nPPV = TP/(TP+FP)\n# Negative predictive value\nNPV = TN/(TN+FN)\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\n# False negative rate\nFNR = FN/(TP+FN)\n# False discovery rate",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "PPV",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "PPV = TP/(TP+FP)\n# Negative predictive value\nNPV = TN/(TN+FN)\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\n# False negative rate\nFNR = FN/(TP+FN)\n# False discovery rate\nFDR = FP/(TP+FP)\n# Overall accuracy",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "NPV",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "NPV = TN/(TN+FN)\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\n# False negative rate\nFNR = FN/(TP+FN)\n# False discovery rate\nFDR = FP/(TP+FP)\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\nOmission_Rate = FN / (FN + TN)",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "FPR",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "FPR = FP/(FP+TN)\n# False negative rate\nFNR = FN/(TP+FN)\n# False discovery rate\nFDR = FP/(TP+FP)\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\nOmission_Rate = FN / (FN + TN)\nprint(\"FNR: \",FNR)\nprint(\"Omission_Rate: \",Omission_Rate)",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "FNR",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "FNR = FN/(TP+FN)\n# False discovery rate\nFDR = FP/(TP+FP)\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\nOmission_Rate = FN / (FN + TN)\nprint(\"FNR: \",FNR)\nprint(\"Omission_Rate: \",Omission_Rate)\n# %%\n# %%",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "FDR",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "FDR = FP/(TP+FP)\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\nOmission_Rate = FN / (FN + TN)\nprint(\"FNR: \",FNR)\nprint(\"Omission_Rate: \",Omission_Rate)\n# %%\n# %%\n# results.reset_index(inplace=True)\n# %%",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "ACC",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "ACC = (TP+TN)/(TP+FP+FN+TN)\nOmission_Rate = FN / (FN + TN)\nprint(\"FNR: \",FNR)\nprint(\"Omission_Rate: \",Omission_Rate)\n# %%\n# %%\n# results.reset_index(inplace=True)\n# %%\n# %%\nresults= results[[\"roc_auc\",\"F2\"]]",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "Omission_Rate",
        "kind": 5,
        "importPath": "scripts.evalute_with_knowlage",
        "description": "scripts.evalute_with_knowlage",
        "peekOfCode": "Omission_Rate = FN / (FN + TN)\nprint(\"FNR: \",FNR)\nprint(\"Omission_Rate: \",Omission_Rate)\n# %%\n# %%\n# results.reset_index(inplace=True)\n# %%\n# %%\nresults= results[[\"roc_auc\",\"F2\"]]\n# %%",
        "detail": "scripts.evalute_with_knowlage",
        "documentation": {}
    },
    {
        "label": "Clear_until_first_texter",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "def Clear_until_first_texter(row):\n     row[\"text\"]=row[\n         \"text\"][(row[\"text\"].texter == 0).idxmax():]\n     row[\"number of rows\"]=len(row[\"text\"].index)\n     return row\n# df=temp_df.apply(lambda row : Clear_until_first_texter(row), axis = 1)\n# df=df[df[\"number of rows\"]>10]\ndef Clear_until_first_texter2(row):\n    row.reset_index(drop=True, inplace=True)\n    # display(row)",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "Clear_until_first_texter2",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "def Clear_until_first_texter2(row):\n    row.reset_index(drop=True, inplace=True)\n    # display(row)\n    row.texter= row.texter.astype(int)\n    row=row[(row.texter == 0).idxmax():]\n    # if len(row.index)>10:\n    row=CombineRows(row)\n    return row\n    # else:\ndef CombineRows(text_df):",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "CombineRows",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "def CombineRows(text_df):\n  text_df[\"del\"]=False\n  texter_col=text_df.columns.get_loc('texter')\n  text_col=text_df.columns.get_loc('text')\n  del_col=text_df.columns.get_loc('del')\n  for i in range(len(text_df)-2,0,-1):\n    if text_df.iloc[i+1,texter_col]== text_df.iloc[i,texter_col] :\n      text_df.iloc[i,text_col]=text_df.iloc[i,text_col]+\" \"+ text_df.iloc[i+1][\"text\"]\n      text_df.iloc[i+1,del_col]=True\n  text_df=text_df[text_df[\"del\"]==False]",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "Clear_until_first_texter",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "def Clear_until_first_texter(row):\n    # display(row)\n    row[\"text\"]=Clear_until_first_texter2(row[\"Transcript\"])\n    row[\"number of rows\"]=len(row[\"text\"].index)\n    return row\n# df=temp_df.apply(lambda row : Clear_until_first_texter(row),axis = 1)\n# Clear_until_first_texter2(temp_df.iloc[0].Transcript)\ndf[\"GSR\"]=df[\"GSR\"].astype(float).astype(int)\ndf.loc[(df.IMSR==1) & (df.GSR==0), 'GSR'] = 1\n# %%",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "check_contains",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "def check_contains(converasion, lex_dict):\n    for k,v in lex_dict.items():\n        to_return=[]\n        # print(converasion)\n        for text in converasion.text:\n            tokens= text.split(\" \")\n            flag=False\n            for feature in v:\n                if flag==False and feature.issubset(tokens):\n                    flag=True",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "check_contains2",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "def check_contains2(converasion, lex_dict):\n    for k,v in lex_dict.items():\n        to_return=[]\n        # print(converasion)\n        for text in converasion.text:\n            flag=False\n            to_return.append(int(flag))\n        converasion[k]=to_return\n    return converasion\ndf[\"Transcript\"]=df.Transcript.progress_apply(lambda x: check_contains2(x,lex_dict)) ",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "compact_dialog",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "def compact_dialog(conv):\n    conv[\"texter1\"]=conv[\"texter\"].map({0:\"A\",\"False\":\"A\",\"True\":\"B\",1:\"B\",False:\"A\",True:\"B\"})\n    # print(conv[\"texter\"]\n    # conv[\"features\"]= [np.zeros((1, 1)) for i in range(len(conv.index))]\n    conv[\"features\"]=conv[left_colums[:-1]].astype(str).apply(','.join, axis=1)\n    # conv[\"features\"]=conv[\"gender\"].astype(str).apply(','.join, axis=1)\n    lst=conv[[\"texter1\",\"text\",\"features\"]].to_records(index=False)\n    dialog = {'knowledge': '', 'utts': lst}\n    return dialog\ndef get_Sahar_data_turns(df):",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "get_Sahar_data_turns",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "def get_Sahar_data_turns(df):\n    dialogs=[] \n    for i in range(len(df.index)):\n        dialog=df.iloc[i].Transcript\n        if len(dialog.index)<20:\n            continue\n        res= compact_dialog(dialog)\n        res[\"GSR\"]=df.iloc[i].GSR\n        res[\"IMSR\"]=df.iloc[i].IMSR\n        # res[\"VED\"]=df.iloc[i][\"Engagement ID\"]",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "conv_saver",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "def conv_saver(dialogs, tokenizer, output_path):\n    \"\"\"binarize data and save the processed data into a hdf5 file\n       :param dialogs: an array of dialogs, \n        each element is a list of <caller, utt, feature> where caller is a string of \"A\" or \"B\",\n        utt is a sentence, feature is an 2D numpy array \n    \"\"\"\n    preprosses_dialogs=[]\n    labels= []\n    for i, dialog in enumerate(tqdm(dialogs)):\n        # print(dialog)",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "src_file_path",
        "kind": 5,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "src_file_path = inspect.getfile(lambda: None)\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(src_file_path))))\nimport pandas as pd \nimport numpy as np\nimport pickle\nfrom transformers import BertTokenizerFast\nimport tables\nimport random\nimport numpy as np\nimport argparse",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "alephbert_tokenizer",
        "kind": 5,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "alephbert_tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\n# alephbert_tokenizer = BertTokenizerFast.f rom_pretrained('onlplab/alephbert-base')\n# with open('/home/izmaylov/Thesis/Preprocess/data/September_Dataset/result_combine.pkl', 'rb') as f:\n#     temp_df = pickle.load(f)\nwith open('/home/izmaylov/Thesis/Preprocess/data/Jan22/labeld_with_IMSR.pkl', 'rb') as f:\n    df = pickle.load(f)\ndef Clear_until_first_texter(row):\n     row[\"text\"]=row[\n         \"text\"][(row[\"text\"].texter == 0).idxmax():]\n     row[\"number of rows\"]=len(row[\"text\"].index)",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "train_out_path",
        "kind": 5,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "train_out_path = os.path.join(data_dir, \"train.pkl\")\ntrain_data_binary=conv_saver(train_data, alephbert_tokenizer, train_out_path)\n# train_out_path = os.path.join(data_dir, \"train_un.pkl\")\n# with open(train_out_path, 'wb') as f:\n#         pickle.dump(train_data, f)\nvalid_out_path = os.path.join(data_dir, \"valid.pkl\")\nvalid_data_binary=conv_saver(valid_data, alephbert_tokenizer, valid_out_path)\n# train_out_path = os.path.join(data_dir, \"valid_un.pkl\")\n# with open(valid_out_path, 'wb') as f:\n#         pickle.dump(valid_data, f)",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "valid_out_path",
        "kind": 5,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "valid_out_path = os.path.join(data_dir, \"valid.pkl\")\nvalid_data_binary=conv_saver(valid_data, alephbert_tokenizer, valid_out_path)\n# train_out_path = os.path.join(data_dir, \"valid_un.pkl\")\n# with open(valid_out_path, 'wb') as f:\n#         pickle.dump(valid_data, f)\ntest_out_path = os.path.join(data_dir, \"test.pkl\")\ntest_data_binary=conv_saver(test_data, alephbert_tokenizer, test_out_path)\n# test_out_path = os.path.join(data_dir, \"test_un.pkl\")\n# with open(test_out_path, 'wb') as f:\n#         pickle.dump(test_data, f)",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "test_out_path",
        "kind": 5,
        "importPath": "scripts.prepare_Data_wtih_knowladge",
        "description": "scripts.prepare_Data_wtih_knowladge",
        "peekOfCode": "test_out_path = os.path.join(data_dir, \"test.pkl\")\ntest_data_binary=conv_saver(test_data, alephbert_tokenizer, test_out_path)\n# test_out_path = os.path.join(data_dir, \"test_un.pkl\")\n# with open(test_out_path, 'wb') as f:\n#         pickle.dump(test_data, f)\n# conv_saver(test_data, alephbert_tokenizer, test_out_path)[1]\n# %%\nt=pickle.load(open(\"data/Sahar_balanced/train.pkl\", \"rb\"))\nlabels=t[0]\ndialogs=t[1]",
        "detail": "scripts.prepare_Data_wtih_knowladge",
        "documentation": {}
    },
    {
        "label": "seed_everything",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "description": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "peekOfCode": "def seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\nseed_everything(42)\ndef check_contains2(converasion, lex_dict):\n    for k,v in lex_dict.items():\n        to_return=[]\n        # print(converasion)\n        for text in converasion.text:\n            flag=False",
        "detail": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "documentation": {}
    },
    {
        "label": "check_contains2",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "description": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "peekOfCode": "def check_contains2(converasion, lex_dict):\n    for k,v in lex_dict.items():\n        to_return=[]\n        # print(converasion)\n        for text in converasion.text:\n            flag=False\n            to_return.append(int(flag))\n        converasion[k]=to_return\n    return converasion\n# alephbert_tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')",
        "detail": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "documentation": {}
    },
    {
        "label": "Clear_until_first_texter",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "description": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "peekOfCode": "def Clear_until_first_texter(row):\n     row[\"text\"]=row[\n         \"text\"][(row[\"text\"].texter == 0).idxmax():]\n     row[\"number of rows\"]=len(row[\"text\"].index)\n     return row\n# df=temp_df.apply(lambda row : Clear_until_first_texter(row), axis = 1)\n# df=df[df[\"number of rows\"]>10]\ndef Clear_until_first_texter2(row):\n    row.reset_index(drop=True, inplace=True)\n    # display(row)",
        "detail": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "documentation": {}
    },
    {
        "label": "Clear_until_first_texter2",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "description": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "peekOfCode": "def Clear_until_first_texter2(row):\n    row.reset_index(drop=True, inplace=True)\n    # display(row)\n    row.texter= row.texter.astype(int)\n    row=row[(row.texter == 0).idxmax():]\n    # if len(row.index)>10:\n    row=CombineRows(row)\n    return row\n    # else:\ndef make_imporant_dict(lex_df):",
        "detail": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "documentation": {}
    },
    {
        "label": "make_imporant_dict",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "description": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "peekOfCode": "def make_imporant_dict(lex_df):\n    def return_important(examples):\n        lst=[]\n        for example in examples:\n            example=example.strip()\n            if \"*\"  in example:\n                example=example.replace(\"*\",\"\")\n                lst+=[example]\n        return lst\n    lex_dict={}",
        "detail": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "documentation": {}
    },
    {
        "label": "CombineRows",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "description": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "peekOfCode": "def CombineRows(text_df):\n  text_df[\"del\"]=False\n  texter_col=text_df.columns.get_loc('texter')\n  text_col=text_df.columns.get_loc('text')\n  del_col=text_df.columns.get_loc('del')\n  for i in range(len(text_df)-2,0,-1):\n    if text_df.iloc[i+1,texter_col]== text_df.iloc[i,texter_col] :\n      text_df.iloc[i,text_col]=text_df.iloc[i,text_col]+\" \"+ text_df.iloc[i+1][\"text\"]\n      text_df.iloc[i+1,del_col]=True\n  text_df=text_df[text_df[\"del\"]==False]",
        "detail": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "documentation": {}
    },
    {
        "label": "Clear_until_first_texter",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "description": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "peekOfCode": "def Clear_until_first_texter(row):\n    # display(row)\n    row[\"text\"]=Clear_until_first_texter2(row[\"Transcript\"])\n    row[\"number of rows\"]=len(row[\"text\"].index)\n    return row\ndef make_data_set(df,from_gsr=True,oversample_factor=1, undersample_factor=1,over_sample_augment=\"IMSR\"):\n    def make_imporant_dict(lex_df,important=True):\n        def return_important(examples):\n            lst=[]\n            for example in examples:",
        "detail": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "documentation": {}
    },
    {
        "label": "make_data_set",
        "kind": 2,
        "importPath": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "description": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "peekOfCode": "def make_data_set(df,from_gsr=True,oversample_factor=1, undersample_factor=1,over_sample_augment=\"IMSR\"):\n    def make_imporant_dict(lex_df,important=True):\n        def return_important(examples):\n            lst=[]\n            for example in examples:\n                example=example.strip()\n                if \"*\"  in example:\n                    example=example.replace(\"*\",\"\")\n                    lst+=[example]\n            return lst",
        "detail": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "documentation": {}
    },
    {
        "label": "src_file_path",
        "kind": 5,
        "importPath": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "description": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "peekOfCode": "src_file_path = inspect.getfile(lambda: None)\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(src_file_path))))\nimport pandas as pd \nimport numpy as np\nimport pickle\nfrom transformers import BertTokenizerFast\nimport random\nimport numpy as np\nimport json\nimport os",
        "detail": "scripts.prepare_Data_wtih_knowladge_with_IMSR",
        "documentation": {}
    },
    {
        "label": "Index",
        "kind": 6,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "class Index(tables.IsDescription):\n    pos_utt = tables.Int32Col() # start offset of an utterance\n    res_len = tables.Int32Col() # number of tokens till the end of response\n    ctx_len = tables.Int32Col() # number of tokens from the start of dialog \nclass Labels(tables.IsDescription):\n    GSR = tables.IntCol() # start offset of an utterance\n    IMSR = tables.IntCol() # number of tokens till the end of response\ndef compact_dialog(conv):\n    conv[\"texter\"]=conv[\"texter\"].replace({0:\"A\",\"False\":\"A\",\"True\":\"B\",1:\"B\",False:\"A\",True:\"B\"})\n    # conv[\"features\"]= [np.zeros((1, 1)) for i in range(len(conv.index))]",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "Labels",
        "kind": 6,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "class Labels(tables.IsDescription):\n    GSR = tables.IntCol() # start offset of an utterance\n    IMSR = tables.IntCol() # number of tokens till the end of response\ndef compact_dialog(conv):\n    conv[\"texter\"]=conv[\"texter\"].replace({0:\"A\",\"False\":\"A\",\"True\":\"B\",1:\"B\",False:\"A\",True:\"B\"})\n    # conv[\"features\"]= [np.zeros((1, 1)) for i in range(len(conv.index))]\n    conv[\"features\"]=conv[left_colums].astype(str).apply(','.join, axis=1)\n    lst=conv[[\"texter\",\"text\",\"features\"]].to_records(index=False)\n    dialog = {'knowledge': '', 'utts': lst}\n    return dialog",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "strip_white",
        "kind": 2,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "def strip_white(df):\n    # print(df)\n    df[\"text\"]=df[\"text\"].replace(r'\\s+', ' ', regex=True)\n    return df\ndf.Transcript=df.Transcript.apply(strip_white)\n# %%\nfrom multiprocesspandas import applyparallel\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom tqdm import tqdm",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "count_contains",
        "kind": 2,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "def count_contains(converasion, lex_dict):\n    for k,v in lex_dict.items():\n        to_return=[]\n        for text in converasion.text:\n            n=0\n            for feature in v:\n                if re.search(r\"\\b{}\\b\".format(feature), text):\n                    n+=1\n            to_return.append(n)\n        converasion[k]=to_return",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "compact_dialog",
        "kind": 2,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "def compact_dialog(conv):\n    conv[\"texter\"]=conv[\"texter\"].replace({0:\"A\",\"False\":\"A\",\"True\":\"B\",1:\"B\",False:\"A\",True:\"B\"})\n    # conv[\"features\"]= [np.zeros((1, 1)) for i in range(len(conv.index))]\n    conv[\"features\"]=conv[left_colums].astype(str).apply(','.join, axis=1)\n    lst=conv[[\"texter\",\"text\",\"features\"]].to_records(index=False)\n    dialog = {'knowledge': '', 'utts': lst}\n    return dialog\ndef get_Sahar_data(df):\n    dialogs=[] \n    for i in range(len(df.index)):",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "get_Sahar_data",
        "kind": 2,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "def get_Sahar_data(df):\n    dialogs=[] \n    for i in range(len(df.index)):\n        dialog=df.iloc[i].Transcript\n        if len(dialog.index)<20:\n            continue\n        res= compact_dialog(dialog)\n        # res[\"IMSR\"]=df.iloc[i].IMSR\n        dialogs.append(res)\n    return dialogs",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "binarize",
        "kind": 2,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "def binarize(dialogs, tokenizer, output_path):\n    \"\"\"binarize data and save the processed data into a hdf5 file\n       :param dialogs: an array of dialogs, \n        each element is a list of <caller, utt, feature> where caller is a string of \"A\" or \"B\",\n        utt is a sentence, feature is an 2D numpy array \n    \"\"\"\n    with tables.open_file(output_path, 'w') as f:\n        filters = tables.Filters(complib='blosc', complevel=5)\n        arr_contexts = f.create_earray(f.root, 'sentences', tables.Int32Atom(),shape=(0,),filters=filters)\n        indices = f.create_table(\"/\", 'indices', Index, \"a table of indices and lengths\")",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "src_file_path",
        "kind": 5,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "src_file_path = inspect.getfile(lambda: None)\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(src_file_path))))\nimport pandas as pd \nimport numpy as np\nimport pickle\nfrom transformers import BertTokenizerFast\nimport tables\nimport random\nimport numpy as np\nimport argparse",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "alephbert_tokenizer",
        "kind": 5,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "alephbert_tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\n# with open('/home/izmaylov/Thesis/Preprocess/40K data/Non_labeled.pkl', 'rb') as f:\n#     df = pickle.load(f)\nwith open('/home/izmaylov/Thesis/Preprocess/40K data/Non_labeled_with_id.pkl', 'rb') as f:\n    df = pickle.load(f)\n# %%\ndf.Transcript.iloc[0]\n# %%\n#read json file\nwith open('/home/izmaylov/Thesis/SR_BERT/data/Data_2/id_dict.json', 'rb') as f:",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "df = df[~df['Engagement ID'].isin(exclude_id)]\n# %%\nleft_colums=[' 21      ',\n ' ',\n ' ',\n '  ',\n ' -',\n '/',\n ' - ',]\nlex_df=pd.read_csv(\"/home/izmaylov/Thesis/lexicon_sim/gsr_lex_4.5.csv\")",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "train_out_path",
        "kind": 5,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "train_out_path = os.path.join(data_dir, \"train.h5\")\ntrain_data_binary=binarize(train_data, alephbert_tokenizer, train_out_path)\nvalid_out_path = os.path.join(data_dir, \"valid.h5\")\nvalid_data_binary=binarize(valid_data, alephbert_tokenizer, valid_out_path)\ntest_out_path = os.path.join(data_dir, \"test.h5\")\ntest_data_binary=binarize(test_data, alephbert_tokenizer, test_out_path)\n# # from pathlib import Path\n# data_dir=\"data/Sahar_balanced\"\n# Path(data_dir).mkdir(parents=True, exist_ok=True)\n# train_out_path = os.path.join(data_dir, \"train.pkl\")",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "valid_out_path",
        "kind": 5,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "valid_out_path = os.path.join(data_dir, \"valid.h5\")\nvalid_data_binary=binarize(valid_data, alephbert_tokenizer, valid_out_path)\ntest_out_path = os.path.join(data_dir, \"test.h5\")\ntest_data_binary=binarize(test_data, alephbert_tokenizer, test_out_path)\n# # from pathlib import Path\n# data_dir=\"data/Sahar_balanced\"\n# Path(data_dir).mkdir(parents=True, exist_ok=True)\n# train_out_path = os.path.join(data_dir, \"train.pkl\")\n# train_data_binary=conv_saver(train_data, alephbert_tokenizer, train_out_path)\n# valid_out_path = os.path.join(data_dir, \"valid.pkl\")",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "test_out_path",
        "kind": 5,
        "importPath": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "description": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "peekOfCode": "test_out_path = os.path.join(data_dir, \"test.h5\")\ntest_data_binary=binarize(test_data, alephbert_tokenizer, test_out_path)\n# # from pathlib import Path\n# data_dir=\"data/Sahar_balanced\"\n# Path(data_dir).mkdir(parents=True, exist_ok=True)\n# train_out_path = os.path.join(data_dir, \"train.pkl\")\n# train_data_binary=conv_saver(train_data, alephbert_tokenizer, train_out_path)\n# valid_out_path = os.path.join(data_dir, \"valid.pkl\")\n# valid_data_binary=conv_saver(valid_data, alephbert_tokenizer, valid_out_path)\n# test_out_path = os.path.join(data_dir, \"test.pkl\")",
        "detail": "scripts.prepare_pre_training_Data_40K_with_lex4.5",
        "documentation": {}
    },
    {
        "label": "BertForSequenceClassification_v2",
        "kind": 6,
        "importPath": "Untitled-1",
        "description": "Untitled-1",
        "peekOfCode": "class BertForSequenceClassification_v2(BertForSequenceClassification):\n    def __init__(self, config):\n        super().__init__(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.init_weights()\n        self.post_init()\n    def forward(\n        self,\n        input_ids=None,",
        "detail": "Untitled-1",
        "documentation": {}
    },
    {
        "label": "DialogTransformerDataset",
        "kind": 6,
        "importPath": "data_loader",
        "description": "data_loader",
        "peekOfCode": "class DialogTransformerDataset(data.Dataset):\n    \"\"\"\n    A base class for Transformer dataset\n    \"\"\"\n    def __init__(self, file_path, tokenizer, \n                 min_num_utts=1, max_num_utts=7, max_utt_len=30, \n                 block_size=256, utt_masklm=False, utt_sop=False, \n                 context_shuf=False, context_masklm=False,with_feature=7):\n        # 1. Initialize file path or list of file names.\n        \"\"\"read training sentences(list of int array) from a hdf5 file\"\"\"",
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "HBertMseEuopDataset",
        "kind": 6,
        "importPath": "data_loader",
        "description": "data_loader",
        "peekOfCode": "class HBertMseEuopDataset(DialogTransformerDataset):\n    \"\"\"\n    A hierarchical Bert data loader where the context is masked with ground truth utterances and to be trained with MSE matching.\n    The context is shuffled for a novel energy-based order prediction approach (EUOP)\n    \"\"\"\n    def __init__(self, file_path, tokenizer,\n                 min_num_utts=1, max_num_utts=9, max_utt_len=30, \n                 block_size=-1, utt_masklm=False, utt_sop=False, \n                 context_shuf=False, context_masklm=False,with_feature=False):\n        self.max_num_utts= max_num_utts",
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "DialogTransformerDatasetClassification",
        "kind": 6,
        "importPath": "data_loader",
        "description": "data_loader",
        "peekOfCode": "class DialogTransformerDatasetClassification(DialogTransformerDataset):\n    \"\"\"\n    A base class for Transformer dataset\n    \"\"\"\n    def __init__(self, file_path, tokenizer, \n                 min_num_utts=1, max_num_utts=7, max_utt_len=30, \n                 block_size=256, utt_masklm=False, utt_sop=False, \n                 context_shuf=False, context_masklm=False,turns=False,with_feature=False,number_labels=2):\n        # 1. Initialize file path or list of file names.\n        \"\"\"read training sentences(list of int array) from a hdf5 file\"\"\"",
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "load_dict",
        "kind": 2,
        "importPath": "data_loader",
        "description": "data_loader",
        "peekOfCode": "def load_dict(filename):\n    return json.loads(open(filename, \"r\").readline())\ndef load_vecs(fin):         \n    \"\"\"read vectors (2D numpy array) from a hdf5 file\"\"\"\n    h5f = tables.open_file(fin)\n    h5vecs= h5f.root.vecs\n    vecs=np.zeros(shape=h5vecs.shape,dtype=h5vecs.dtype)\n    vecs[:]=h5vecs[:]\n    h5f.close()\n    return vecs",
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "load_vecs",
        "kind": 2,
        "importPath": "data_loader",
        "description": "data_loader",
        "peekOfCode": "def load_vecs(fin):         \n    \"\"\"read vectors (2D numpy array) from a hdf5 file\"\"\"\n    h5f = tables.open_file(fin)\n    h5vecs= h5f.root.vecs\n    vecs=np.zeros(shape=h5vecs.shape,dtype=h5vecs.dtype)\n    vecs[:]=h5vecs[:]\n    h5f.close()\n    return vecs\ndef save_vecs(vecs, fout):\n    fvec = tables.open_file(fout, 'w')",
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "save_vecs",
        "kind": 2,
        "importPath": "data_loader",
        "description": "data_loader",
        "peekOfCode": "def save_vecs(vecs, fout):\n    fvec = tables.open_file(fout, 'w')\n    atom = tables.Atom.from_dtype(vecs.dtype)\n    filters = tables.Filters(complib='blosc', complevel=5)\n    ds = fvec.create_carray(fvec.root,'vecs', atom, vecs.shape,filters=filters)\n    ds[:] = vecs\n    print('done')\n    fvec.close()",
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "data_loader",
        "description": "data_loader",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass DialogTransformerDataset(data.Dataset):\n    \"\"\"\n    A base class for Transformer dataset\n    \"\"\"\n    def __init__(self, file_path, tokenizer, \n                 min_num_utts=1, max_num_utts=7, max_utt_len=30, \n                 block_size=256, utt_masklm=False, utt_sop=False, \n                 context_shuf=False, context_masklm=False,with_feature=7):\n        # 1. Initialize file path or list of file names.",
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "DialogBert_Infernce",
        "kind": 6,
        "importPath": "inference",
        "description": "inference",
        "peekOfCode": "class DialogBert_Infernce():\n    def  __init__(self,lexicon=0, with_feature=0,IMSR=1,number_labels=3):\n        args ={'data_path': './data/',\"lexicon\":0,\"IMSR\":IMSR,\"number_labels\":number_labels, 'dataset': 'Sahar_labeld', 'fine_tune': 1,\"train_base\":0, 'model': 'DialogBERT', 'model_size': 'base', 'language': 'hebrew', 'per_gpu_train_batch_size': 16, 'per_gpu_eval_batch_size': 32, 'grad_accum_steps': 1, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'n_epochs': 20.0, 'max_steps': -1, 'warmup_steps': 1, 'version': 'unbalanced_7_long', 'version_load': 'pretrain/noFreezing_30K_long', 'reload_from': 67500, 'max_num_utts': 40, 'early_stop': -1.0, 'logging_steps': 9999, 'validating_steps': 265, 'save_steps': 1500, 'save_total_limit': 2, 'seed': 42, 'fp16': False, 'fp16_opt_level': 'O1', 'local_rank': -1, 'server_ip': '', 'server_port': '',\"feature_loss_alpha\":1.0, \"with_turns\":1}\n        args[\"lexicon\"]=lexicon\n        args[\"with_feature\"]=with_feature\n        args= Namespace(**args)\n        args.data_path = os.path.join(args.data_path, args.dataset)\n        # Setup CUDA, GPU & distributed training\n        if args.local_rank == -1:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
        "detail": "inference",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "inference",
        "description": "inference",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)        \nclass DialogBert_Infernce():\n    def  __init__(self,lexicon=0, with_feature=0,IMSR=1,number_labels=3):\n        args ={'data_path': './data/',\"lexicon\":0,\"IMSR\":IMSR,\"number_labels\":number_labels, 'dataset': 'Sahar_labeld', 'fine_tune': 1,\"train_base\":0, 'model': 'DialogBERT', 'model_size': 'base', 'language': 'hebrew', 'per_gpu_train_batch_size': 16, 'per_gpu_eval_batch_size': 32, 'grad_accum_steps': 1, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'n_epochs': 20.0, 'max_steps': -1, 'warmup_steps': 1, 'version': 'unbalanced_7_long', 'version_load': 'pretrain/noFreezing_30K_long', 'reload_from': 67500, 'max_num_utts': 40, 'early_stop': -1.0, 'logging_steps': 9999, 'validating_steps': 265, 'save_steps': 1500, 'save_total_limit': 2, 'seed': 42, 'fp16': False, 'fp16_opt_level': 'O1', 'local_rank': -1, 'server_ip': '', 'server_port': '',\"feature_loss_alpha\":1.0, \"with_turns\":1}\n        args[\"lexicon\"]=lexicon",
        "detail": "inference",
        "documentation": {}
    },
    {
        "label": "Metrics",
        "kind": 6,
        "importPath": "learner",
        "description": "learner",
        "peekOfCode": "class Metrics:\n    def __init__(self):\n        super(Metrics, self).__init__()\n        '''\n        self.rouge_evaluator = rouge.Rouge(metrics=['rouge-l'],\n                           max_n=4,\n                           limit_length=True,\n                           length_limit=200,\n                           length_limit_type='words',\n                           apply_avg=True,",
        "detail": "learner",
        "documentation": {}
    },
    {
        "label": "Learner",
        "kind": 6,
        "importPath": "learner",
        "description": "learner",
        "peekOfCode": "class Learner(object):\n    def run_train(self, args, model, train_set, optim_params, entry='forward', max_steps = -1, \n                  do_validate=True, valid_set=None, do_test=True, test_set=None):         \n        tb_writer=None\n        best_model=None\n        best_F2=0\n        best_F1=0\n        best_loss=99900\n        early_stop=False\n        # for p in model.parameters():",
        "detail": "learner",
        "documentation": {}
    },
    {
        "label": "parentPath",
        "kind": 5,
        "importPath": "learner",
        "description": "learner",
        "peekOfCode": "parentPath = os.path.abspath(\"..\")\nsys.path.insert(0, parentPath)# add parent folder to path so as to import common modules\nfrom transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup)\nimport models, data_loader\nfrom data_loader import DialogTransformerDataset, load_vecs\n#import rouge # pip install py-rouge\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score",
        "detail": "learner",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "learner",
        "description": "learner",
        "peekOfCode": "logger = logging.getLogger(__name__)    \nclass Learner(object):\n    def run_train(self, args, model, train_set, optim_params, entry='forward', max_steps = -1, \n                  do_validate=True, valid_set=None, do_test=True, test_set=None):         \n        tb_writer=None\n        best_model=None\n        best_F2=0\n        best_F1=0\n        best_loss=99900\n        early_stop=False",
        "detail": "learner",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)        \ndef main():\n    nltk.download('wordnet')\n    parser = argparse.ArgumentParser()\n    ## Required parameters",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def main():\n    nltk.download('wordnet')\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--data_path\", default='./data/', type=str, help=\"The input data path.\")\n    # parser.add_argument(\"--dataset\", default='Sahar_labeld', type=str, help=\"dataset name\")\n    parser.add_argument(\"--dataset\", default='Sahar_2_with_age', type=str, help=\"dataset name\")\n    # parser.add_argument(\"--dataset\", default='Sahar_labeld', type=str, help=\"dataset name\")\n    # parser.add_argument(\"--dataset\", default='Sahar_small', type=str, help=\"dataset name\")\n    parser.add_argument(\"--fine_tune\", default=1, type=int, help=\"Fine tuning the model for classification task\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)        \ndef main():\n    nltk.download('wordnet')\n    parser = argparse.ArgumentParser()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "main_pretrain",
        "description": "main_pretrain",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)        \ndef main():\n    # nltk.download('wordnet')\n    parser = argparse.ArgumentParser()\n    ## Required parameters",
        "detail": "main_pretrain",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "main_pretrain",
        "description": "main_pretrain",
        "peekOfCode": "def main():\n    # nltk.download('wordnet')\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--data_path\", default='./data/', type=str, help=\"The input data path.\")\n    parser.add_argument(\"--dataset\", default='Sahar_30K_4.5', type=str, help=\"dataset name\")\n    parser.add_argument(\"--fine_tune\", default=0, type=int, help=\"Fine tuning the model for classification  task\")\n    parser.add_argument(\"--train_base\", default=0, type=int, help=\"Train the base model or not\")\n    parser.add_argument(\"--lexicon\", default=0, type=int, help=\"Train the base model or not\")\n    parser.add_argument(\"--with_feature\", default=7, type=int, help=\"Train the base model or not\")",
        "detail": "main_pretrain",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "main_pretrain",
        "description": "main_pretrain",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)        \ndef main():\n    # nltk.download('wordnet')\n    parser = argparse.ArgumentParser()",
        "detail": "main_pretrain",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "modules",
        "description": "modules",
        "peekOfCode": "class MLP(nn.Module):\n    def __init__(self, input_size, arch, output_size, activation=nn.ReLU(), batch_norm=True, init_w=0.02, discriminator=False):\n        super(MLP, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.init_w= init_w\n        if type(arch) == int: arch= str(arch) # simple integer as hidden size\n        layer_sizes = [input_size] + [int(x) for x in arch.split('-')]\n        self.layers = []\n        for i in range(len(layer_sizes)-1):",
        "detail": "modules",
        "documentation": {}
    },
    {
        "label": "BertForSequenceClassification_v2",
        "kind": 6,
        "importPath": "modules",
        "description": "modules",
        "peekOfCode": "class BertForSequenceClassification_v2(BertForSequenceClassification):\n    def __init__(self, config):\n        super().__init__(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.init_weights()\n        self.post_init()\n    def forward(\n        self,\n        input_ids=None,",
        "detail": "modules",
        "documentation": {}
    },
    {
        "label": "BertForSequenceClassification_lex_features",
        "kind": 6,
        "importPath": "modules",
        "description": "modules",
        "peekOfCode": "class BertForSequenceClassification_lex_features(BertForSequenceClassification):\n    def __init__(self,config):\n        super().__init__(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size +5, config.num_labels)\n        self.init_weights()\n        self.post_init()\n    def forward(\n        self,\n        input_ids=None,",
        "detail": "modules",
        "documentation": {}
    },
    {
        "label": "MixtureDensityNetwork",
        "kind": 6,
        "importPath": "modules",
        "description": "modules",
        "peekOfCode": "class MixtureDensityNetwork(nn.Module):\n    \"\"\"\n    Mixture density network. [Bishop, 1994]. Adopted from https://github.com/tonyduan/mdn\n    References: \n        http://cbonnett.github.io/MDN.html\n        https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca\n    ----------\n    dim_in: int; dimensionality of the covariates\n    dim_out: int; dimensionality of the response variable\n    n_components: int; number of components in the mixture model",
        "detail": "modules",
        "documentation": {}
    },
    {
        "label": "parentPath",
        "kind": 5,
        "importPath": "modules",
        "description": "modules",
        "peekOfCode": "parentPath = os.path.abspath(\"..\")\nsys.path.insert(0, parentPath)# add parent folder to path so as to import common modules\nclass MLP(nn.Module):\n    def __init__(self, input_size, arch, output_size, activation=nn.ReLU(), batch_norm=True, init_w=0.02, discriminator=False):\n        super(MLP, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.init_w= init_w\n        if type(arch) == int: arch= str(arch) # simple integer as hidden size\n        layer_sizes = [input_size] + [int(x) for x in arch.split('-')]",
        "detail": "modules",
        "documentation": {}
    },
    {
        "label": "Index",
        "kind": 6,
        "importPath": "prepare_data",
        "description": "prepare_data",
        "peekOfCode": "class Index(tables.IsDescription):\n    pos_utt = tables.Int32Col() # start offset of an utterance\n    res_len = tables.Int32Col() # number of tokens till the end of response\n    ctx_len = tables.Int32Col() # number of tokens from the start of dialog \ndef binarize(dialogs, tokenizer, output_path):\n    \"\"\"binarize data and save the processed data into a hdf5 file\n       :param dialogs: an array of dialogs, \n        each element is a list of <caller, utt, feature> where caller is a string of \"A\" or \"B\",\n        utt is a sentence, feature is an 2D numpy array \n    \"\"\"",
        "detail": "prepare_data",
        "documentation": {}
    },
    {
        "label": "binarize",
        "kind": 2,
        "importPath": "prepare_data",
        "description": "prepare_data",
        "peekOfCode": "def binarize(dialogs, tokenizer, output_path):\n    \"\"\"binarize data and save the processed data into a hdf5 file\n       :param dialogs: an array of dialogs, \n        each element is a list of <caller, utt, feature> where caller is a string of \"A\" or \"B\",\n        utt is a sentence, feature is an 2D numpy array \n    \"\"\"\n    f = tables.open_file(output_path, 'w')\n    filters = tables.Filters(complib='blosc', complevel=5)\n    arr_contexts = f.create_earray(f.root, 'sentences', tables.Int32Atom(),shape=(0,),filters=filters)\n    indices = f.create_table(\"/\", 'indices', Index, \"a table of indices and lengths\")",
        "detail": "prepare_data",
        "documentation": {}
    },
    {
        "label": "get_daily_dial_data",
        "kind": 2,
        "importPath": "prepare_data",
        "description": "prepare_data",
        "peekOfCode": "def get_daily_dial_data(data_path):\n    dialogs = []\n    dials = open(data_path, 'r').readlines()\n    for dial in dials:\n        utts = []\n        for i, utt in enumerate(dial.rsplit(' __eou__ ')):\n            caller = 'A' if i % 2 == 0 else 'B'\n            utts.append((caller, utt, np.zeros((1, 1))))\n        dialog = {'knowledge': '', 'utts': utts}\n        dialogs.append(dialog)",
        "detail": "prepare_data",
        "documentation": {}
    },
    {
        "label": "get_multiwoz_data",
        "kind": 2,
        "importPath": "prepare_data",
        "description": "prepare_data",
        "peekOfCode": "def get_multiwoz_data(data_path):\n    timepat = re.compile(\"\\d{1,2}[:]\\d{1,2}\")\n    pricepat = re.compile(\"\\d{1,3}[.]\\d{1,2}\")\n    def normalize(text):\n        text = text.lower()\n        text = re.sub(r'^\\s*|\\s*$', '', text)# replace white spaces in front and end\n        # hotel domain pfb30\n        text = re.sub(r\"b&b\", \"bed and breakfast\", text)\n        text = re.sub(r\"b and b\", \"bed and breakfast\", text)\n        # normalize phone number",
        "detail": "prepare_data",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "prepare_data",
        "description": "prepare_data",
        "peekOfCode": "def load_data(data_path, data_name):\n    data={'train':[],'valid':[], 'test':[]}\n    if args.data_set=='dailydial':\n        data['train'] = get_daily_dial_data(\"dialogues_train.txt\")\n        # data['valid'] = get_daily_dial_data(data_path+'valid.utts.txt')\n        # data['test'] = get_daily_dial_data(data_path+'test.utts.txt')\n    elif args.data_set=='multiwoz':\n        train, valid, test = get_multiwoz_data(os.path.join(data_path, 'data.json'))\n        data['train'] = train\n        data['valid'] = valid",
        "detail": "prepare_data",
        "documentation": {}
    },
    {
        "label": "get_args",
        "kind": 2,
        "importPath": "prepare_data",
        "description": "prepare_data",
        "peekOfCode": "def get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-d', \"--data_set\", default='dailydial', help='multiwoz, dailydial')\n    parser.add_argument('-m', \"--model_name\",default='bert-base-uncased', help='bert-base-uncased')\n    return parser.parse_args()\nif __name__ == \"__main__\":\n    args=get_args()\n    # work_dir = \"./data/\"\n    # data_dir = work_dir + args.data_set+'/'\n    # print(\"loading data...\")",
        "detail": "prepare_data",
        "documentation": {}
    },
    {
        "label": "DialogBERTSolver",
        "kind": 6,
        "importPath": "solvers",
        "description": "solvers",
        "peekOfCode": "class DialogBERTSolver(object):\n    def __init__(self, args, model=None):\n        self.model = model    \n        self.build(args)\n    def build(self, args):\n        # Load pretrained model and tokenizer\n        if args.local_rank not in [-1, 0]:\n            torch.distributed.barrier() # make sure only the first process in distributed training download model & vocab\n        if self.model is None:\n            # self.model = DialogBERT(args)    ",
        "detail": "solvers",
        "documentation": {}
    },
    {
        "label": "get_optim_params",
        "kind": 2,
        "importPath": "solvers",
        "description": "solvers",
        "peekOfCode": "def get_optim_params(models, args):\n    no_decay = ['bias', 'LayerNorm.weight']\n    parameters = []\n    for model in models:\n        parameters.append(\n            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n             'weight_decay': args.weight_decay})\n        parameters.append(\n            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n             'weight_decay': 0.0})",
        "detail": "solvers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "solvers",
        "description": "solvers",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef get_optim_params(models, args):\n    no_decay = ['bias', 'LayerNorm.weight']\n    parameters = []\n    for model in models:\n        parameters.append(\n            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n             'weight_decay': args.weight_decay})\n        parameters.append(\n            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "solvers",
        "documentation": {}
    }
]